{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Assignment5_Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OijTajZMW0"
      },
      "source": [
        "\n",
        "# Primer\n",
        "\n",
        "## Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "## The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bseyTwlZMXz"
      },
      "source": [
        "## Overview\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFhXB_DZMX1"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X44kNHcxZMX4"
      },
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n",
        "\n",
        "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
        "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
        "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
        "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n",
        "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n",
        "\n",
        "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n",
        "\n",
        "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_0 \\in S$, which is also known as the *value* for that state, \n",
        "\n",
        "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n",
        "\n",
        "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n",
        "\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
        "\n",
        "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is\n",
        "\n",
        "$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n",
        "\n",
        "It turns out that a solution to the dynamic programming equation, known as the *Bellman equation*, is an optimal policy. The Bellman equation is given by\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
        "\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
        "\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_JnoKlxZMX5"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aPdvf3wZMX6"
      },
      "source": [
        "The first question covers a deterministic MPD, described as follows:\n",
        "\n",
        "* The agent starts in state **S**\n",
        "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
        "* The transition probabilities in each box are uniform. Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n",
        "* When reaching **F**, the game ends (absorbing state).\n",
        "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
        "* Assume no discount in this model: $\\gamma = 1$\n",
        "    \n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|-1 |1|**F**|\n",
        "|0|-1|1|  \n",
        "|-1 |0|-1|  \n",
        "|**S**|-1|1|\n",
        "\n",
        "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
        "\n",
        "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n",
        "\n",
        "**1b)** What is the optimal policy (i.e. the optimal action in each state)?\n",
        "\n",
        "**1c)** What is expected total reward for the policy in 1b)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlQPG3XeW-fE"
      },
      "source": [
        "**1a**) One of the optimal paths of the MDP is the path EENNN. It is not however a unique optimal path since there are cycles that have a reward of zero per cycle like (1,0)->(2,0)->(1,0) or (2,2)->(1,2)->(2,2). Due to this, a path like EEWEWENNN will also be an optimal path.\n",
        "\n",
        "**1b) and 1c)** Much like the optimal path, there is no unique optimal policy. However, one of the optimal policies is shown below. The letter in position $(x,y)$ is the action given from $\\pi((x,y))$.\n",
        "\n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|E |E|**F**|\n",
        "|E|E|N|  \n",
        "|E |E|N|  \n",
        "|E|E|N|\n",
        "\n",
        "To show that this policy is an optimal policy, we simply look at $V^{\\pi}(s), s\\in S$ for each state and check that it fulfills the Bellman equation for all steps. For example:\n",
        "\n",
        "$$V^\\pi(1,2)= 1 = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s') + V^\\pi(s')) \\right\\} =\\max(1+0,1+0,0+0,0+0),$$\n",
        "where the first equality comes from following the path determined by the policy and the last equality comes from following the path laid out by our policy for each position adjacent to (1,2) and adding the reward from going to the new position. By evaluating the Bellman Equation for all other positions, we find that it is fulfilled for all states. By then evaluating $V^\\pi(S)$ we find that the expected reward is $-1+1-1+1=0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvrrH7gmZMX8"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5lm25HZMYB"
      },
      "source": [
        "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DVWiG0jZMYD"
      },
      "source": [
        "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probabilit 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
        "Action **S**: 0  \n",
        "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **W**: 0\n",
        "\n",
        "Hence any action between **E** and **N** would be best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msBJNydkZMYH"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n",
        "\n",
        "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sb4OU5N09v8"
      },
      "source": [
        "2a) In this task we have coded the iteration algorithm described above and found convergin value iteration. The Value iteration matrix converged in 145 iteration. In order to apply the algorithm we have wrote two functions. The function **possible_actions** helps to identify possible actions that we can make in any state. For instance, according to the function in the cell of (0,0), which is the first row and column of the matrix,  we can only go East and South. On the other hand, the next function which actually applies the algorithm is function **V** which is value iteration function. This is a recursive function where we loop through all the cells of matrix, find possible actions for each and apply formula according to each state-action pair. Finally at the end of each iteration we take a maximum of actions for each cell.\n",
        "\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "According to the formula, the function itself is recursive. In each iteration we also use the previous value iteration matrix. While we are not done, means our V matrix did not converged yet, we call our **V** function again. When previous V matrix is the same with updated V matrix, we return the updated matrix with best actions for each state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2i0LVOjbyUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "42c2494c-f8db-4d38-df3e-966d53f21b03"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def possible_actions(i, j, matrix_size):\n",
        "    '''\n",
        "    function helps to find all possible actions for each state\n",
        "    '''\n",
        "    pos_actions = []\n",
        "    #check S\n",
        "    if i+1 < matrix_size:\n",
        "        pos_actions.append('S')\n",
        "    #check N\n",
        "    if i-1 > 0:\n",
        "        pos_actions.append('N')\n",
        "    #check E\n",
        "    if j+1 < matrix_size:\n",
        "        pos_actions.append('E')\n",
        "    #check W\n",
        "    if j-1 >= 0:\n",
        "        pos_actions.append('W')\n",
        "    return pos_actions\n",
        "\n",
        "\n",
        "def V(reward_matrix, prev_matrix, updated_matrix, iteration_number):\n",
        "    '''\n",
        "    recursive function that calls itself until converging to the optimal Value matrix\n",
        "    Args:\n",
        "        reward_matrix (np.array): table of rewards in each state\n",
        "        prev_matrix (np.array): matrix of previous value iteration\n",
        "        updated_matrix (np.array): matrix of new value iteration\n",
        "        iteration_number (int): counter that helps to keep track of iteration number\n",
        "    Returns:\n",
        "        updated matrix or recursive function\n",
        "    '''\n",
        "\n",
        "    final_actions = {}\n",
        "    for i in range(len(reward_matrix)):\n",
        "        for j in range(len(reward_matrix)):\n",
        "            actions = possible_actions(i, j, len(reward_matrix))\n",
        "\n",
        "            best_scores = []\n",
        "            best_action = []\n",
        "            for action in actions:\n",
        "                if action == 'S':\n",
        "                    V_value = 0.8*(reward_matrix[i+1,j] + 0.9 * prev_matrix[i+1,j]) +\\\n",
        "                        0.2*(reward_matrix[i,j] + 0.9 * prev_matrix[i,j])\n",
        "                    best_scores.append(V_value); best_action.append('S')\n",
        "                elif action == 'N':\n",
        "                    V_value = 0.8*(reward_matrix[i-1,j] + 0.9 * prev_matrix[i-1,j]) +\\\n",
        "                        0.2*(reward_matrix[i,j] + 0.9 * prev_matrix[i,j])\n",
        "                    best_scores.append(V_value); best_action.append('N')\n",
        "                elif action == 'E':\n",
        "                    V_value = 0.8*(reward_matrix[i,j+1] + 0.9 * prev_matrix[i,j+1]) +\\\n",
        "                        0.2*(reward_matrix[i,j] + 0.9 * prev_matrix[i,j])\n",
        "                    best_scores.append(V_value); best_action.append('E')\n",
        "                elif action == 'W':\n",
        "                    V_value = 0.8*(reward_matrix[i,j-1] + 0.9 * prev_matrix[i,j-1]) +\\\n",
        "                        0.2*(reward_matrix[i,j] + 0.9 * prev_matrix[i,j])\n",
        "                    best_scores.append(V_value); best_action.append('W')\n",
        "\n",
        "                # update value iteration matrix and store final actions\n",
        "                updated_matrix[i, j] = max(best_scores)\n",
        "                final_actions[(i, j)] = best_action[best_scores.index(max(best_scores))]\n",
        "\n",
        "    if (prev_matrix == updated_matrix).all():\n",
        "        print(\"Value function converged to the optimal in\", iteration_number, \" iteration\")\n",
        "        print(\"\\nValue function matrix is \\n\", updated_matrix)\n",
        "        print(\"\\nBest actions for each index: \\n\", final_actions)\n",
        "        return updated_matrix\n",
        "    else:\n",
        "        prev_matrix = updated_matrix\n",
        "        updated_matrix = np.zeros((3,3), dtype=np.float32)\n",
        "        return V(reward_matrix, prev_matrix, updated_matrix, iteration_number+1)\n",
        "\n",
        "\n",
        "value_matrix = np.zeros((3, 3), dtype=np.float32)\n",
        "updated_matrix = np.zeros((3, 3), dtype=np.float32)\n",
        "reward_matrix = np.array([[0, 0, 0], [0, 10, 0], [0, 0, 0]], dtype=np.float32)\n",
        "\n",
        "V_matrix = V(reward_matrix, value_matrix, updated_matrix, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value function converged to the optimal in 145  iteration\n",
            "\n",
            "Value function matrix is \n",
            " [[45.612915 51.948044 45.612915]\n",
            " [51.948044 48.05194  51.948044]\n",
            " [45.612915 51.948044 45.612915]]\n",
            "\n",
            "Best actions for each index: \n",
            " {(0, 0): 'S', (0, 1): 'S', (0, 2): 'S', (1, 0): 'E', (1, 1): 'S', (1, 2): 'W', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBof50KV423y"
      },
      "source": [
        "As you can see from above table we get converged optimal V function in 145 iteration. And the best actions for each state has been listed above. It makes sense, the reward is in the cell of (1, 1), and if we keep track of best actions we can observe that each cell tries to reach out to the cell of (1, 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-VXkteL6nKD"
      },
      "source": [
        "2b)\n",
        "The result of Value iteration does not depend on initial state. We can show this by proof.\n",
        "For any estimate of $\\hat{V}$ value function, let $B:R^{|S|}→R^{|S|}$ be a Bellman backup operator then we define following equation:\n",
        "\n",
        "$$ B \\hat{V}(s) = R(s) + \\gamma \\max_{a\\in A} \\sum_{s'\\in S} P(s'|s,a) \\hat{V}(s')$$\n",
        "\n",
        "Since Bellman operator is contraction, we can say that for any value function estimates $V_1, V_2$\n",
        "\n",
        "$$\\max_{s \\in S}|B V_1(s) - B V_2(s)| \\leq \\gamma \\max_{s \\in S}|B V_1(s) - B V_2(s)|  $$ \n",
        "\n",
        "Since $B V^* = V^*$, the contraction property also implies existence and uniqueness of the fixed point, we have:\n",
        "$$\\max_{s \\in S}|B V_1(s) - B V_2(s)| \\leq \\gamma \\max_{s \\in S}|B V_1(s) - B V_2(s)| \\implies \\hat{V}→V^*  $$\n",
        "\n",
        "\n",
        "By showing this proof we can say that if the Bellman operator is a contraction for every value function, then $V^*$ is a fixed point which every other $V$ will converge to. Therefore result of the value function does not depend on the \n",
        "whether initial state is 0 or not.\n",
        "\n",
        "Proof of contraction property:\n",
        "$$|BV_1(s) - BV_2| = \\gamma|\\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a) V_1(s') - \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a) V_2(s')|$$\n",
        "\n",
        "$$\\leq |\\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a) V_1(s') - \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a) V_2(s')|$$\n",
        "\n",
        "$$ = |\\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a) V_1(s') - V_2(s')|$$\n",
        "\n",
        "$$ \\leq \\gamma|\\max_{s \\in S} V_1(s') - V_2(s')|$$\n",
        "\n",
        "The third line comes from following property:\n",
        "$$|\\max_{x} f(x) - \\max_{x} g(x)| \\leq \\max_{x}|f(x) - g(x)|$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " <!-- As we let the initial value $V_0$ to be 0, the result of 2a is not dependent from initial value. After letting $V_0$ to be zero, the formula changes and it becomes like following:\n",
        "\n",
        " $$V(s) = \\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$$\n",
        "\n",
        " So the value iteration matrix will not be dependent on $V_0$, but it will depend on the $V_1$ and also other iterations. As we said the original value iteration matrix we also use previous iteration matrix, However in the first iteration we do not have a previous matrix iteration to use. So that`s why it does not depend on initial value iteration $V_0$. -->\n",
        "\n",
        "\n",
        "\n",
        " <!-- As we let the initial value $V_0$ to be 0, the result of 2a is not dependent from initial value. After letting $V_0$ to be zero, the formula changes and it becomes like following:\n",
        "\n",
        " $$V(s) = \\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$$\n",
        "\n",
        " So the value iteration matrix will not be dependent on $V_0$, but it will depend on the $V_1$ and also other iterations. As we said the original value iteration matrix we also use previous iteration matrix, However in the first iteration we do not have a previous matrix iteration to use. So that`s why it does not depend on initial value iteration $V_0$. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRxCmNyXZMYJ"
      },
      "source": [
        "## Reinforcement Learning (RL)\n",
        "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
        "\n",
        "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "The value function and the action-value function are directly related through\n",
        "\n",
        "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
        "\n",
        "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
        "\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
        "\n",
        "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n",
        "\n",
        "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
        "\n",
        "## Question 3\n",
        "You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Take $\\gamma=0.95$. You can refer to the Q-learning Jupyter notebook shown in class, uploaded on Canvas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5zTp25WI5p"
      },
      "source": [
        "**3** In order to decide which action to take for each step of the Q-learning algorithm, we use a parameter called exploit which determines the percentage of times that the action will follow the action with the maximum Q-value. In all other cases, it will choose it's action randomly. After taking a step with the determined action, we update the entry with our previous state and current action according to the algorithm and continue for 10000 iterations as that should be enough time for convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW_TzOJwkg68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fdf65477-093a-421c-aa45-609ea4b096d8"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "#Create environment and Q-table\n",
        "env = gym.make('NChain-v0')\n",
        "Q=np.ones((env.action_space.n,env.observation_space.n))\n",
        "#Initialize exploitation rate and learning rate\n",
        "exploit=0.8\n",
        "alpha=0.1\n",
        "gamma=0.95\n",
        "#Collect starting state and a random initial action\n",
        "observation=env.reset()\n",
        "action=env.action_space.sample()\n",
        "trail=0\n",
        "trails=10000\n",
        "#Until the simulation is considered wrong\n",
        "while(trail<trails):\n",
        "    #Randomize whether we exploit or explore\n",
        "    trail+=1\n",
        "    ran=np.random.uniform()\n",
        "    #If we explore\n",
        "    if(ran<exploit):\n",
        "        #Pick the action that has the highest Q-value, reverse during ties.\n",
        "        if(Q[0,observation]>observation):\n",
        "            action=0\n",
        "        else:\n",
        "            action=1\n",
        "    #If we explore\n",
        "    else:\n",
        "        #Pick action randomly\n",
        "        action=env.action_space.sample()\n",
        "    #Take a new action\n",
        "    newObservation, reward, done, info = env.step(action)\n",
        "    #Adjust the Q-value\n",
        "    QPrev=Q[action][observation]\n",
        "    Q[action][observation]+=alpha*(reward+gamma*np.maximum(Q[0],Q[1])[newObservation]-Q[action][observation])\n",
        "    #Check if the Q-value has changed\n",
        "    if(QPrev == Q[action][observation]):\n",
        "        break\n",
        "    observation=newObservation\n",
        "    observation=newObservation\n",
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[59.75344622 62.8664392  65.91317838 70.25393346 81.82632813]\n",
            " [59.55962739 60.99998703 61.64733598 63.73534082 66.48172399]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q33elK7WHpB"
      },
      "source": [
        "The table above represents the Q-values for each state and action when the algorithm stops, with the top row representing the values when moving forwards and the bottom representing moving backwards. Unfortunately, the table does not fully converge to a single value due to the stochastic nature of the Q-learning algorithm. The values instead oscillate around the same value every time we run the algorithm. Otherwise the values make a lot of sense. Going forward is expected to give a larger value than going backwards and the value is also expected to increase as we are closer to the end of the chain. It's obvious that this happens when we move forward, but the reason why this also happens when moving backwards is that the program has a chance to slip and move forwards instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei6_pOFSkeQ-"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "**4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. \n",
        "\n",
        "**4b)** What is the importance of exploration in RL? Explain with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4a8p6OwMd9m"
      },
      "source": [
        "**4a)** The corresponding MDP to the chain environment is defined to have the state space $S=\\{1,2,3,4,5\\}$, the action space $A=\\{F,B\\}$, where $F$ is the action of moving forwards and $B$ is the action of moving backwards, the reward function $R(a,s,s')$ is defined by $R(a,s,1)=2$,$R(a,5,5)=10$ and $R(a,s,s')=0$ for all other $s$ and $s'$. Meanwhile the transition probabilities $p(s'|a,s)$ is defined as $p(1|0,s)=0.2$, $p(1|1,s)=0.8$, $p(min(s+1,5)|0,s)= 0.8$, $p(min(s+1,5)|1,s)= 0.2$ and $p(s'|a,s)=0$ for all other $s$, $a$ and $s'$. By using a modified version of our value iteration algorithm on this MDP, we get the following results after 327 iterations when $\\gamma=0.95$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKDqjbCBQ_WR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "735cc885-01e5-4c80-be24-26a842811fc8"
      },
      "source": [
        "import numpy as np\n",
        "v0=np.zeros(5)\n",
        "gamma=0.95\n",
        "#Calculates the expected value function and policy for the chain problem with initial expected value v0 and discount factor gamma\n",
        "def chainIteration(v0,gamma):\n",
        "    r=np.array([[0,0,0,0,10],[2,2,2,2,2]])\n",
        "    vOld=np.zeros(np.size(v0))\n",
        "    vNew=v0\n",
        "    maxAct=np.chararray(np.size(v0))\n",
        "    trail=0\n",
        "    #Until the state is stable\n",
        "    while(True):\n",
        "        trail+=1\n",
        "        #Copy the values from the previous iteration\n",
        "        vOld = vNew.copy()\n",
        "        #For each position\n",
        "        for i in range(np.size(v0)):\n",
        "            maxValue=np.NINF\n",
        "            #Check the expected value going forward\n",
        "            value=0.8*(r[0,i]+gamma*vOld[min(i+1,np.size(v0)-1)])+0.2*(r[1,i]+gamma*vOld[0])\n",
        "            if(value>maxValue):\n",
        "                vNew[i]=value\n",
        "                maxValue=value\n",
        "                maxAct[i]='F'\n",
        "            #Check the expected value going backwards     \n",
        "            value=0.8*(r[1,i]+gamma*vOld[0])+0.2*(r[0,i]+gamma*vOld[min(i+1,np.size(v0)-1)])\n",
        "            if(value>maxValue):\n",
        "                vNew[i]=value\n",
        "                maxValue=value\n",
        "                maxAct[i]='B'\n",
        "        #Break if no change has been made.\n",
        "        if(vNew==vOld).all():\n",
        "            break\n",
        "    return(vNew,maxAct,trail)\n",
        "value, policy, trail = chainIteration(v0,gamma)\n",
        "print(trail)\n",
        "print(value)\n",
        "print(policy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "679\n",
            "[61.3794816 64.8912896 69.5120896 75.5920896 83.5920896]\n",
            "[b'F' b'F' b'F' b'F' b'F']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NciSsMlkRIsu"
      },
      "source": [
        "We see here that the optimal policy is to always try moving forward, and that the optimal value function $V^*(s)$ is fairly close to the top row of our previous Q-table, which represented the expected value after moving forward, thus confirming that $V^*(s) = \\max_{a\\in A} Q^*(s,a)$. Note once again that due to the randomness of $Q$, our Q-table is not quite the same as $V^*(s)$.\n",
        "\n",
        "**4b)** The reason why exploration is important in reinforcement learning is that we sometimes need to look into other options than the option that is known to be best in order to truly determine the optimal option at a state. \n",
        "\n",
        "An example of exploration in real life is when trying to buying food at a store. Here, we might always pick the most valuable option that we know when buying a certain food, based on a mixture of price, quality and distance, but it might be good to sometimes check another store that you've never visited before. It might be more expensive or have worse wares than the best one you know, but then you can just go back to the old store the next time with no big harm done. If it turns out that the store is cheaper and better, you can start going to this store instead of the old one and get more value than before. You might also find that some stores are better to shop when it is raining or when you want to buy a certain item."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Leys_tqGZMZV"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
        "\n",
        "\n",
        "5a) Decision tree is a tree-shaped diagram where nodes represent questions, branches represent answers and leaf nodes represent class labels. In the Decision tree, we have one tree that has a root node. To built a Decision tree, we should first find the best feature to put on a root node. By meaning the best feature, we should keep track of impurity for each feature of the dataset. One of the methods that we can use is the Gini score.\n",
        "\n",
        "$$Gini = 1 - P(Yes)^2 - P(No)^2$$\n",
        "\n",
        "We compute the Gini score for each feature in the dataset and we take the feature as a root node which has the lowest Gini score.\n",
        "Then recursively we apply the same steps for further nodes. After using another feature in a node, if the lowest value for the Gini score is higher than the previous one we should stop because we are getting more impurities in our splitting, which makes the result worse. On the other hand, we should stop when the leaf nodes are pure, which means all the labels should be $Yes$ or $No$.\n",
        "\n",
        "However, there are also negative sides of decision trees. They are not flexible when predicting new samples, can overfit easily, Inaccurate.\n",
        "\n",
        "There is also Random Forest which is set of decision trees. To avoid overfitting and to have vast improvement random forest are vital to use. To build a random forest model, we should create a bootstrapped dataset by taking samples from the original data with replacemet. After this, we build a descision tree from the bootstrapped data but choose a random subset of columns to use for each branch, rather than using all columns. For instance, we use 4 random columns from the dataset to construct the root as before and then construct the next branch from 3 columns that are unused and so on. After this, we construct a new bootstrapped dataset and create another decision tree from it and repeat until we have built a large number of trees. To predict a new sample, we first predict the sample using those decision trees, where we got while building the random forest. We take the label which has the most votes. For instance, if we have built 10 decision trees using random columns, while predicting the new sample we get 7 $yes$ 3 $no$ label. Then obviously we choose $yes$ label since it has the highest vote. Random forest is so advantageous rather than decision trees. They are flexible to the new sample, as we have many decision trees. On the other hand, we can avoid overfitting by using Random Forest. Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONDrnno123h6"
      },
      "source": [
        "**5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n",
        "\n",
        "Reinforcement learning differs from the supervised learning in a way that in Supervised learning there is a data and labels for each instance. We train a model which maps from data to labels. So in supervised learning we have a data and we make a training over this data. So by fitting the data we can predict a new sample. However, in reinforcement learning no training can be done over the data since we gather data by making an actions. Agent learns from its own experience. Reinforcement learning is sequential decision making process. In reinforcement learning we have agent, states and rewards. Each time agent takes an action in the particular state, we either penalize him or give a reward depending on action that he took."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZcqRx4ZMZW"
      },
      "source": [
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ]
    }
  ]
}